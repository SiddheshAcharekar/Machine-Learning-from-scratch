{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Neural Network Classification for supervised tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done on 2 datasets: MNIST, and 20 NG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) For MNIST dataset, run a TF in supervised mode (train/test) and report results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siddhesh Acharekar\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "'''\n",
    "Go through batches of 100 images at a time\n",
    "'''\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    '''\n",
    "    Create a tensor(array) of weights that are initialized randomly.\n",
    "    Shape is previous layer columns vs New layer rows\n",
    "    Then add biases\n",
    "    formula is actually: ipdata*weights + biases\n",
    "    Why bias? If ipdata is 0 then 0*weights is 0 so no neuron would ever fire\n",
    "    More importantly it allows you to shift the sigmoid function left or right \n",
    "    '''\n",
    "\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "#     hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "#                       'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\n",
    "    # model is: ip_data*weights + biases\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    #activation function\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    #activation function\n",
    "    l2 = tf.nn.relu(l2) \n",
    "\n",
    "#     l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "#     #activation function\n",
    "#     l3 = tf.nn.relu(l3) \n",
    "\n",
    "    output = tf.add(tf.matmul(l2, output_layer['weights']), output_layer['biases'])\n",
    "    #activation function\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    # You take data, pass it through your neural network\n",
    "    prediction = neural_network_model(x)\n",
    "    # print(prediction)\n",
    "    #Minimize cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction,labels = y))\n",
    "    \n",
    "    # print('cost shape is: ',cost.shape)\n",
    "    # Learning rate parameter default is 0.001\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Epochs are cycles of feedfwd and backprop\n",
    "    hm_epochs = 10\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                # print('epoch_y is:', epoch_y[0])\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})    \n",
    "                # print(\"c is: \",c)\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch ', epoch, 'completed out of ', hm_epochs, 'loss: ', epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy: ',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        tf.train.write_graph(sess.graph, '/tmp/my-model', 'train.pbtxt')\n",
    "        # writer = tf.summary.FileWriter('tmp/mnist_demo/l', sess.graph)\n",
    "        # writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 completed out of  10 loss:  127353.01651954651\n",
      "Epoch  1 completed out of  10 loss:  32358.983506679535\n",
      "Epoch  2 completed out of  10 loss:  18996.794564102267\n",
      "Epoch  3 completed out of  10 loss:  12035.141606745708\n",
      "Epoch  4 completed out of  10 loss:  7732.487982865587\n",
      "Epoch  5 completed out of  10 loss:  5088.2801134075635\n",
      "Epoch  6 completed out of  10 loss:  3440.4356208906593\n",
      "Epoch  7 completed out of  10 loss:  2368.1608387052447\n",
      "Epoch  8 completed out of  10 loss:  1878.0957595156779\n",
      "Epoch  9 completed out of  10 loss:  1392.8818865765697\n",
      "Accuracy:  0.9535\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) TF classification for 20NG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "ng_train = fetch_20newsgroups(subset = 'train')          # 11,314 datapoints in .data and category targets in .target_names\n",
    "ng_test = fetch_20newsgroups(subset = 'test')\n",
    "\n",
    "# use nltk's stopwords to reduce matrix dimensions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopw = list(set(stopwords.words('english')))\n",
    "\n",
    "# Convert to tf-idf vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_20ng_class = TfidfVectorizer(stop_words=stopw)\n",
    "tfidf_20ng_train = tfidf_20ng_class.fit_transform(ng_train.data)\n",
    "tfidf_20ng_test = tfidf_20ng_class.fit_transform(ng_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bychi_2000 = SelectKBest(chi2, 2000)\n",
    "# bychi_4000 = SelectKBest(chi2, 4000)\n",
    "# new_train_2000 = bychi_2000.fit_transform(X_train, y_train)\n",
    "# new_train_4000 = bychi_4000.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bychi_800 = SelectKBest(chi2, 800)\n",
    "new_train_800 = bychi_800.fit_transform(tfidf_20ng_train, ng_train.target)\n",
    "# new_train_800 = bychi_800.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 800)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_800.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_train_800, ng_train.target, test_size = 0.3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7919, 800), (3395, 800))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels as one-hot vectors\n",
    "\n",
    "y_train_onehot = np.zeros((len(y_train), 20))\n",
    "for sample,target in enumerate(y_train):\n",
    "    y_train_onehot[sample,target] = 1\n",
    "    \n",
    "y_test_onehot = np.zeros((len(y_test), 20))\n",
    "for sample,target in enumerate(y_test):\n",
    "    y_test_onehot[sample,target] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7919, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data also in top features by chi2\n",
    "new_test_2000 = bychi_2000.fit_transform(X_test, y_test)\n",
    "new_test_4000 = bychi_4000.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_800 = bychi_800.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "# n_nodes_hl3 = 200\n",
    "\n",
    "n_classes = 20\n",
    "# Set batch size parameter if needed here\n",
    "# x placeholder parameter changes size by input features\n",
    "# For 20ng try with 2000 and 4000\n",
    "x = tf.placeholder('float', [None, 800])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([800, n_nodes_hl1])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "#     hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "#                       'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "\n",
    "    # model is: ip_data*weights + biases\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    #activation function\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    #activation function\n",
    "    l2 = tf.nn.relu(l2) \n",
    "\n",
    "#     l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "#     #activation function\n",
    "#     l3 = tf.nn.relu(l3) \n",
    "\n",
    "    output = tf.add(tf.matmul(l2, output_layer['weights']), output_layer['biases'])\n",
    "    #activation function\n",
    "    return output\n",
    "\n",
    "\n",
    "def train_neural_network(x):\n",
    "    # You take data, pass it through your neural network\n",
    "    prediction = neural_network_model(x)\n",
    "    # print(prediction)\n",
    "    #Minimize cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction,labels = y))\n",
    "    \n",
    "    # print('cost shape is: ',cost.shape)\n",
    "    # Learning rate parameter default is 0.001\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate= 0.05).minimize(cost)\n",
    "\n",
    "    # Epochs are cycles of feedfwd and backprop\n",
    "    hm_epochs = 30\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_x, epoch_y = X_train.todense(), y_train_onehot\n",
    "#             print (epoch_x)\n",
    "            # print('epoch_y is:', epoch_y[0])\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})    \n",
    "            # print(\"c is: \",c)\n",
    "            epoch_loss += c\n",
    "\n",
    "            print('Epoch ', epoch, 'completed out of ', hm_epochs, 'loss: ', epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy on train data: ',accuracy.eval({x:X_train.todense(), y:y_train_onehot}))\n",
    "        print('Accuracy on test data: ',accuracy.eval({x:X_test.todense(), y:y_test_onehot}))\n",
    "#         tf.train.write_graph(sess.graph, '/tmp/my-model', 'train.pbtxt')\n",
    "        # writer = tf.summary.FileWriter('tmp/mnist_demo/l', sess.graph)\n",
    "        # writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 completed out of  30 loss:  880.1112670898438\n",
      "Epoch  1 completed out of  30 loss:  727.9601440429688\n",
      "Epoch  2 completed out of  30 loss:  747.3934326171875\n",
      "Epoch  3 completed out of  30 loss:  603.26123046875\n",
      "Epoch  4 completed out of  30 loss:  637.5509643554688\n",
      "Epoch  5 completed out of  30 loss:  693.1651000976562\n",
      "Epoch  6 completed out of  30 loss:  753.481689453125\n",
      "Epoch  7 completed out of  30 loss:  737.0881958007812\n",
      "Epoch  8 completed out of  30 loss:  767.10205078125\n",
      "Epoch  9 completed out of  30 loss:  731.1504516601562\n",
      "Epoch  10 completed out of  30 loss:  644.152099609375\n",
      "Epoch  11 completed out of  30 loss:  578.3634643554688\n",
      "Epoch  12 completed out of  30 loss:  491.4989929199219\n",
      "Epoch  13 completed out of  30 loss:  378.7703857421875\n",
      "Epoch  14 completed out of  30 loss:  313.6956787109375\n",
      "Epoch  15 completed out of  30 loss:  268.4210205078125\n",
      "Epoch  16 completed out of  30 loss:  228.1151885986328\n",
      "Epoch  17 completed out of  30 loss:  204.36842346191406\n",
      "Epoch  18 completed out of  30 loss:  170.36273193359375\n",
      "Epoch  19 completed out of  30 loss:  148.69667053222656\n",
      "Epoch  20 completed out of  30 loss:  115.31838989257812\n",
      "Epoch  21 completed out of  30 loss:  87.99357604980469\n",
      "Epoch  22 completed out of  30 loss:  73.44792938232422\n",
      "Epoch  23 completed out of  30 loss:  63.97588348388672\n",
      "Epoch  24 completed out of  30 loss:  55.049407958984375\n",
      "Epoch  25 completed out of  30 loss:  45.245948791503906\n",
      "Epoch  26 completed out of  30 loss:  36.14788818359375\n",
      "Epoch  27 completed out of  30 loss:  29.177101135253906\n",
      "Epoch  28 completed out of  30 loss:  23.811073303222656\n",
      "Epoch  29 completed out of  30 loss:  20.63370132446289\n",
      "Accuracy on train data:  0.69983584\n",
      "Accuracy on test data:  0.6783505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n800 features, 2 hidden layers: 500,500, learning rate = 0.05, epochs increased to 30\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neural_network(x)\n",
    "'''\n",
    "800 features, 2 hidden layers: 500,500, learning rate = 0.05, epochs increased to 30\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 completed out of  20 loss:  1253.2384033203125\n",
      "Epoch  1 completed out of  20 loss:  769.3799438476562\n",
      "Epoch  2 completed out of  20 loss:  553.9244384765625\n",
      "Epoch  3 completed out of  20 loss:  385.3398742675781\n",
      "Epoch  4 completed out of  20 loss:  345.66326904296875\n",
      "Epoch  5 completed out of  20 loss:  334.6177978515625\n",
      "Epoch  6 completed out of  20 loss:  324.4775390625\n",
      "Epoch  7 completed out of  20 loss:  291.6800231933594\n",
      "Epoch  8 completed out of  20 loss:  244.28057861328125\n",
      "Epoch  9 completed out of  20 loss:  200.14328002929688\n",
      "Epoch  10 completed out of  20 loss:  166.7349853515625\n",
      "Epoch  11 completed out of  20 loss:  142.88552856445312\n",
      "Epoch  12 completed out of  20 loss:  129.06190490722656\n",
      "Epoch  13 completed out of  20 loss:  114.88496398925781\n",
      "Epoch  14 completed out of  20 loss:  99.55828857421875\n",
      "Epoch  15 completed out of  20 loss:  86.62059020996094\n",
      "Epoch  16 completed out of  20 loss:  76.98542785644531\n",
      "Epoch  17 completed out of  20 loss:  69.8436508178711\n",
      "Epoch  18 completed out of  20 loss:  63.92903137207031\n",
      "Epoch  19 completed out of  20 loss:  57.59666442871094\n",
      "Accuracy on train data:  0.7935345\n",
      "Accuracy on test data:  0.0730486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n4000 features, 2 hidden layers: 1000,500, learning rate = 0.01\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neural_network(x)\n",
    "'''\n",
    "4000 features, 2 hidden layers: 1000,500, learning rate = 0.01\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 completed out of  20 loss:  6783.3173828125\n",
      "Epoch  1 completed out of  20 loss:  4102.3125\n",
      "Epoch  2 completed out of  20 loss:  3539.541015625\n",
      "Epoch  3 completed out of  20 loss:  2651.954345703125\n",
      "Epoch  4 completed out of  20 loss:  2285.2412109375\n",
      "Epoch  5 completed out of  20 loss:  2144.851806640625\n",
      "Epoch  6 completed out of  20 loss:  1708.6185302734375\n",
      "Epoch  7 completed out of  20 loss:  1507.900390625\n",
      "Epoch  8 completed out of  20 loss:  1334.896484375\n",
      "Epoch  9 completed out of  20 loss:  1026.6300048828125\n",
      "Epoch  10 completed out of  20 loss:  820.61669921875\n",
      "Epoch  11 completed out of  20 loss:  737.2379150390625\n",
      "Epoch  12 completed out of  20 loss:  647.441162109375\n",
      "Epoch  13 completed out of  20 loss:  558.7777099609375\n",
      "Epoch  14 completed out of  20 loss:  491.4356689453125\n",
      "Epoch  15 completed out of  20 loss:  413.13800048828125\n",
      "Epoch  16 completed out of  20 loss:  343.8560791015625\n",
      "Epoch  17 completed out of  20 loss:  295.7493896484375\n",
      "Epoch  18 completed out of  20 loss:  253.61587524414062\n",
      "Epoch  19 completed out of  20 loss:  219.02914428710938\n",
      "Accuracy on train data:  0.73443615\n",
      "Accuracy on test data:  0.088954344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n2000 features, 3 hidden layers: 1000,500,200 learning rate = 0.01\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neural_network(x)\n",
    "'''\n",
    "2000 features, 3 hidden layers: 1000,500,200 learning rate = 0.01\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 completed out of  10 loss:  6814.07763671875\n",
      "Epoch  1 completed out of  10 loss:  5207.626953125\n",
      "Epoch  2 completed out of  10 loss:  3456.850341796875\n",
      "Epoch  3 completed out of  10 loss:  2797.771240234375\n",
      "Epoch  4 completed out of  10 loss:  2700.213134765625\n",
      "Epoch  5 completed out of  10 loss:  2011.58251953125\n",
      "Epoch  6 completed out of  10 loss:  1559.55029296875\n",
      "Epoch  7 completed out of  10 loss:  1342.8489990234375\n",
      "Epoch  8 completed out of  10 loss:  1239.013671875\n",
      "Epoch  9 completed out of  10 loss:  1033.87255859375\n",
      "Accuracy on train data:  0.55903524\n",
      "Accuracy on test data:  0.070103094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n4000 features, 3 hidden layers: 1000,500,200 learning rate = 0.01\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neural_network(x)\n",
    "'''\n",
    "4000 features, 3 hidden layers: 1000,500,200 learning rate = 0.01\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "With a learning rate of 0.1 it was overshooting with an accuracy of 80%, with 4000 features and learning rate 0.01 the \n",
    "accuracy is 87% and doesnt overshoot.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
